# Batch 9: Advanced NLU with Fine-Tuned BERT - Implementation Guide

## Executive Summary

Batch 9 implements an intelligent Natural Language Understanding (NLU) service that classifies medical queries into 7 intent categories using a fine-tuned BiomedBERT model. This enables the CareDroid system to intelligently route user queries to the appropriate clinical tools, emergency protocols, or AI-assisted responses.

**Status**: âœ… Phase 1 Complete (Infrastructure) | â³ Phase 2-4 Pending (Training, Integration, Testing)

---

## Architecture Overview

### System Components

```
User Query (Mobile/Web)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend (NestJS Node.js)           â”‚
â”‚   â† Routes queries to NLU service    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   NLU Service (FastAPI Python 8001)  â”‚
â”‚   â”œâ”€ Intent Classification            â”‚
â”‚   â”œâ”€ Confidence Scoring               â”‚
â”‚   â””â”€ Emergency Subcategory Detection  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    â”œâ”€ Confidence â‰¥ 0.7 â†’ Use NLU Result
    â””â”€ Confidence < 0.7 â†’ Fallback to GPT-4
```

### Service Specifications

| Component | Details |
|-----------|---------|
| **Framework** | FastAPI (async ASGI) |
| **Language** | Python 3.11+ |
| **Base Model** | BiomedBERT (Microsoft) |
| **Port** | 8001 |
| **GPU/CPU** | Auto-detection with CUDA support |
| **Inference** | Batch processing (1-100 texts) |
| **Latency Target** | <50ms (p95) |
| **Accuracy Target** | >90% across all intents |

---

## Intent Classes

### 1. Emergency (High Priority)
High-risk medical situations requiring immediate escalation.

**Subcategories**:
- **Cardiac**: Chest pain, MI, arrhythmias, heart failure
- **Neurological**: Stroke, seizure, loss of consciousness, severe headache
- **Respiratory**: Severe dyspnea, choking, respiratory failure
- **Sepsis**: Fever + hypotension + altered mental status
- **Trauma**: Severe injury, bleeding, fractures
- **Psychiatric**: Suicidal ideation, severe agitation, psychosis

**Example Queries**:
- "Patient has severe chest pain radiating to left arm"
- "What do I do for someone having a seizure?"
- "Patient can't breathe, oxygen sat is 80%"
- "Signs of septic shock in elderly patient"
- "Patient threatening self-harm"

---

### 2. Clinical Tool (Tools & Calculators)
Requests to use clinical decision support tools and calculators.

**Associated Tools**:
- SOFA Score Calculator (organ failure assessment)
- Drug Interaction Checker
- Medication Dosing Calculator
- Lab Value Interpreter

**Example Queries**:
- "Calculate SOFA score for this patient"
- "Check drug interactions for metformin and lisinopril"
- "What's the correct dose of vancomycin for renal impairment?"
- "Explain what this potassium level means"

---

### 3. Lab Query (Lab Value Interpretation)
Queries about lab value interpretation, significance, and normal ranges.

**Topics**:
- Lab value normal ranges
- Interpretation of abnormal results
- Clinical significance
- Trending and monitoring

**Example Queries**:
- "What does this hemoglobin of 7.2 indicate?"
- "Is WBC of 15,000 elevated?"
- "How should I interpret this troponin level?"
- "What do high liver enzymes suggest?"

---

### 4. Protocol Search (Guidelines & Protocols)
Requests for institutional protocols, clinical guidelines, and standard procedures.

**Protocols Include**:
- Sepsis recognition and management
- ACLS (Advanced Cardiac Life Support)
- ATLS (Advanced Trauma Life Support)
- Stroke protocols
- Institutional procedures and order sets

**Example Queries**:
- "Show me the sepsis protocol"
- "What's the ACLS algorithm for VT?"
- "What are our institution's codes for handoff?"
- "When do we culture for line infections?"

---

### 5. General Query (Medical Knowledge)
General medical knowledge, pathophysiology, and educational questions.

**Topics**:
- Disease pathophysiology
- Risk factors and epidemiology
- Treatment mechanisms
- Medical terminology
- Clinical reasoning

**Example Queries**:
- "What causes diabetic nephropathy?"
- "How does ACE inhibitors work?"
- "What are risk factors for stroke?"
- "Explain the pathophysiology of heart failure"

---

### 6. Patient Data (Medical Records)
Queries about accessing, retrieving, or understanding patient data.

**Data Types**:
- Patient medical history
- Medication lists
- Lab results and trends
- Imaging reports
- Problem list and diagnoses

**Example Queries**:
- "Show me this patient's medication list"
- "What labs were done last week?"
- "Pull up imaging results for this patient"
- "What's in the problem list?"

---

### 7. Admin Function (Administrative Tasks)
Administrative and compliance-related functions.

**Functions**:
- HIPAA settings and access logs
- Audit trail review
- Compliance reporting
- User management and permissions
- Data security and encryption status

**Example Queries**:
- "Who accessed this patient's chart?"
- "Generate a HIPAA access report"
- "Set data encryption policy"
- "Review my audit logs"

---

## File Structure

```
backend/ml-services/nlu/
â”œâ”€â”€ README.md                    # Service documentation
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ requirements-dev.txt         # Dev dependencies (pytest, black, etc.)
â”œâ”€â”€ setup.sh                     # Setup script (Linux/macOS)
â”œâ”€â”€ setup.ps1                    # Setup script (Windows)
â”œâ”€â”€ config.py                    # Configuration (models, training, inference)
â”œâ”€â”€ model.py                     # NLUModel class (inference, batch processing)
â”œâ”€â”€ app.py                       # FastAPI service (HTTP endpoints)
â”œâ”€â”€ utils.py                     # Utility functions
â”œâ”€â”€ prepare_data.py              # Data splitting script
â”œâ”€â”€ train.py                     # Model training script
â”œâ”€â”€ evaluate.py                  # Model evaluation script
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model/              # (Created after training)
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â””â”€â”€ tokenizer files
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.jsonl              # Training data (80% after split)
â”‚   â”œâ”€â”€ val.jsonl                # Validation data (10% after split)
â”‚   â””â”€â”€ test.jsonl               # Test data (10% after split)
â””â”€â”€ logs/                        # Training logs
```

---

## Phase 1: Infrastructure Setup âœ… COMPLETE

### Tasks Completed

**1. âœ… Python Service Skeleton** (app.py - 400+ lines)
- FastAPI application with lifespan management
- Endpoints: `/predict`, `/batch-predict`, `/health`, `/model-info`, `/intent-classes`
- Pydantic request/response validation
- Error handling with proper HTTP status codes
- Ready for: Launch with `python -m uvicorn app:app`

**2. âœ… NLUModel Class** (model.py - 350+ lines)
- Methods: `load()`, `predict()`, `predict_batch()`, `_detect_subcategory()`, `get_model_info()`
- Features: GPU/CPU auto-detection, lazy loading, batch processing, latency tracking
- Ready for: Import by app.py for inference

**3. âœ… Configuration Management** (config.py - 170 lines)
- MODEL_CONFIG: Model name, max_length=512, num_labels=7
- TRAINING_CONFIG: 5 epochs, batch_size=16, learning_rate=2e-5
- INFERENCE_CONFIG: batch_size=32, confidence_threshold=0.5
- MODEL_PATHS: Directories for models, data, logs, metrics
- Ready for: Import by train.py, evaluate.py, app.py

**4. âœ… Docker Containerization** (Dockerfile + docker-compose.yml)
- Multi-stage build: Builder (install deps) â†’ Runtime (lean image)
- Integration: NLU service on port 8001 in docker-compose.yml
- Health check: GET /health with 30s interval
- Ready for: `docker build` and `docker-compose up`

**5. âœ… Training Dataset** (data/train.jsonl - 100+ examples)
- Format: JSONL (one JSON per line)
- Structure: {"text": "clinical query", "intent": "intent_name", "subcategory": "(optional)"}
- Distribution: 7 intent classes with balanced examples
- Categories:
  - Emergency: 20 examples (cardiac, neuro, respiratory, sepsis, trauma, psychiatric)
  - Clinical_tool: 20 examples (calculators, drug interactions)
  - Lab_query: 15 examples (interpretation, normal ranges)
  - Protocol_search: 15 examples (guidelines, institutional)
  - General_query: 15 examples (pathophysiology, knowledge)
  - Patient_data: 10 examples (records, history)
  - Admin_function: 10 examples (HIPAA, audit, compliance)
- Ready for: Splitting into train/val/test

**6. âœ… Project Configuration** (.env.example - 28 variables)
- Model configs, training hyperparameters, service settings
- All parameters overridable via environment variables
- Ready for: Copy to `.env` and customize

**7. âœ… Documentation** (README.md)
- Quick start guide
- Setup instructions (Windows, Linux, macOS)
- API endpoint examples
- Configuration guide
- Troubleshooting section
- Ready for: Team reference

**8. âœ… Setup Scripts**
- setup.sh (Linux/macOS)
- setup.ps1 (Windows)
- prepare_data.py (data splitting)
- Ready for: One-command setup

---

## Phase 2: Model Training (Days 3-5) â³ PENDING

### Tasks

**1. â³ Prepare Training Data**

```bash
# Run data preparation script
python prepare_data.py
```

**Output**: 
- data/train.jsonl (80 examples)
- data/val.jsonl (10 examples)
- data/test.jsonl (10 examples)

**Expected Results**:
```
ğŸ“Š Intent Distribution:
  emergency          : 16 (16%)
  clinical_tool      : 16 (16%)
  lab_query          : 12 (12%)
  protocol_search    : 12 (12%)
  general_query      : 12 (12%)
  patient_data       :  8 (8%)
  admin_function     :  8 (8%)
```

**2. â³ Train Model**

```bash
# Start training
python train.py
```

**Expected Duration**:
- GPU (e.g., V100): 15-20 minutes
- CPU: 1-2 hours

**Configuration** (from config.py):
- Model: BiomedBERT from Hugging Face
- Epochs: 5
- Batch Size: 16
- Learning Rate: 2e-5
- Warmup Steps: 500
- Early Stopping: Patience=3

**Outputs**:
- models/best_model/ (checkpoint with best validation loss)
  - config.json (model config)
  - pytorch_model.bin (weights)
  - training_args.bin (hyperparameters)
  - Added tokens.json, special_tokens_map.json, tokenizer.json, tokenizer.model, tokenizer_config.json, vocab.txt
- logs/ (TensorBoard logs)
- Training log in console

**3. â³ Evaluate Model**

```bash
# Evaluate on test set
python evaluate.py
```

**Success Criteria**:
- Overall Accuracy: >90%
- Macro F1: >0.90
- All intent classes: >0.85 F1
- Inference Latency P95: <50ms

**Expected Output**:
```
==================================================
EVALUATION RESULTS
==================================================
Accuracy: 0.9375
Macro F1: 0.9250
Weighted F1: 0.9370
Macro Precision: 0.9400
Macro Recall: 0.9180

Inference Latency (ms):
  P50: 42.32
  P95: 48.15
  P99: 51.87
  Mean: 43.21
Test Set Size: 48
==================================================
```

**Metrics Saved**: models/metrics.json

---

## Phase 3: Backend Integration (Days 6-7) â³ PENDING

### Tasks

**1. â³ Create NLU Client Service**

**File**: backend/src/modules/medical-control-plane/services/nlu-client.service.ts

**Features**:
- HTTP client to NLU service (http://nlu:8001)
- Single and batch prediction methods
- Caching layer (in-memory or Redis)
- Retry logic with exponential backoff (3 retries)
- Connection timeout handling

**Example Method**:
```typescript
async predictIntent(text: string): Promise<PredictionResult> {
  try {
    const response = await this.httpClient.post(
      'http://nlu:8001/predict',
      { text },
      { timeout: 5000 }
    ).toPromise();
    return response;
  } catch (error) {
    this.logger.warn(`NLU service error: ${error.message}`);
    throw error;
  }
}
```

**2. â³ Update IntentClassifier Service**

**File**: backend/src/modules/medical-control-plane/services/intent-classifier.service.ts

**Changes**:
- Inject NLUClientService
- Update classify() method to call NLU service
- Implement fallback logic with confidence threshold

**Logic Flow**:
```typescript
async classifyIntent(text: string): Promise<IntentClassification> {
  try {
    // Call NLU service
    const nluResult = await this.nluClient.predictIntent(text);
    
    // Check confidence threshold
    if (nluResult.confidence >= 0.7) {
      return nluResult; // Use NLU result
    } else {
      // Fallback to GPT-4 for low confidence
      return await this.gpt4Fallback(text);
    }
  } catch (error) {
    // Service unavailable - use GPT-4
    this.logger.warn(`NLU service unavailable: ${error.message}`);
    return await this.gpt4Fallback(text);
  }
}
```

**3. â³ Implement Fallback Strategy**

**Scenarios**:
1. **High Confidence (â‰¥0.7)**: Use NLU result
2. **Low Confidence (<0.7)**: Call GPT-4 with context
3. **Service Unavailable**: Use GPT-4 + keyword matching

**GPT-4 Fallback Prompt**:
```
You are a medical intent classifier. Classify the user's query into one of these categories:

Categories:
- emergency: High-risk medical situations
- clinical_tool: Requests for clinical tools/calculators
- lab_query: Lab value interpretation
- protocol_search: Protocol or guideline lookup
- general_query: Medical knowledge questions
- patient_data: Patient record queries
- admin_function: Administrative tasks

User Query: "{text}"

Respond with JSON: {"intent": "category_name", "confidence": 0.95}
```

**4. â³ Error Handling & Monitoring**

**Implementation**:
- Timeout handling (5s per request)
- Retry logic (exponential backoff: 100ms â†’ 200ms â†’ 400ms)
- Circuit breaker pattern (fail fast after 3 consecutive failures)
- Metrics tracking (success rate, latency, error types)
- Logging of intent predictions and confidence scores

**Metrics to Track**:
- NLU success rate (target: >99%)
- Average latency (target: <100ms)
- Fallback rate (should be <10%)
- Confidence distribution (show % of high/medium/low confidence)

---

## Phase 4: Testing & Benchmarking (Days 8-10) â³ PENDING

### Tasks

**1. â³ Unit Tests**

**File**: backend/ml-services/nlu/tests/

**Test Cases**:
- Model loading (lazy loading verification)
- Single prediction (accuracy, output format)
- Batch prediction (batching, chunking)
- Subcategory detection (emergency subcategories)
- Edge cases (empty text, very long text, special characters)
- GPU/CPU fallback

**Example Test**:
```python
def test_predict_emergency_cardiac():
    model = NLUModel()
    result = model.predict("Severe chest pain radiating to left arm")
    assert result["intent"] == "emergency"
    assert result["subcategory"] == "cardiac"
    assert result["confidence"] > 0.90
```

**2. â³ Integration Tests**

**File**: backend/test/medical-control-plane/intent-classifier.e2e-spec.ts

**Test Scenarios**:
- NLU service communication
- Confidence threshold behavior
- Fallback to GPT-4
- Error handling
- Cache behavior (if implemented)
- Concurrent requests

**3. â³ Performance Benchmarks**

**Metrics to Measure**:
- Single inference latency (p50, p95, p99)
- Batch inference throughput (texts/sec)
- Memory usage (peak and steady state)
- Model loading time

**Benchmark Script** (load_test.py):
```bash
# Test 100 sequential predictions
python load_test.py --sequential --num_requests 100

# Test 50 concurrent predictions
python load_test.py --concurrent --num_requests 50 --workers 10
```

**Target Results**:
- P50 Latency: <40ms
- P95 Latency: <50ms
- P99 Latency: <60ms
- Throughput: >20 texts/sec (batch mode)
- Memory: <2GB GPU, <500MB CPU

**4. â³ Load Testing**

**Tools**: Apache JMeter or Locust

**Scenarios**:
- 10 concurrent users (normal load)
- 50 concurrent users (peak load)
- 100 concurrent users (stress test)

**Success Criteria**:
- Error rate: <0.1%
- 95th percentile latency: <100ms (under load)
- Service availability: >99.9%

**5. â³ Documentation**

**Create**:
- API documentation (Swagger/OpenAPI schema)
- Deployment guide (local, Docker, Kubernetes)
- Troubleshooting guide (common issues and solutions)
- Usage examples for backend integration
- Performance tuning guide

---

## Quick Start Guide

### Prerequisites
- Python 3.11+
- 2GB disk space (for models)
- GPU recommended (CUDA 11.8+ for faster training)

### Setup (5 minutes)

**Windows**:
```powershell
cd backend\ml-services\nlu
.\setup.ps1
```

**Linux/macOS**:
```bash
cd backend/ml-services/nlu
chmod +x setup.sh
./setup.sh
```

### Train Model (1-2 hours CPU, 15-20 min GPU)

```bash
# Activate environment
source venv/bin/activate  # Linux/macOS
.\venv\Scripts\Activate.ps1  # Windows

# Prepare data (2 minutes)
python prepare_data.py

# Train model
python train.py
```

### Evaluate and Deploy

```bash
# Evaluate on test set (5 minutes)
python evaluate.py

# Start service on port 8001
python -m uvicorn app:app --host 0.0.0.0 --port 8001

# Or with Docker
docker build -t caredroid-nlu .
docker run -p 8001:8001 caredroid-nlu
```

### Test Integration

```bash
# Check health
curl http://localhost:8001/health

# Test single prediction
curl -X POST http://localhost:8001/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "Patient has severe chest pain"}'

# Response should be:
# {"intent": "emergency", "confidence": 0.98, "subcategory": "cardiac", ...}
```

---

## Dependencies

### Core (requirements.txt)
- **fastapi==0.100.0**: Web framework
- **uvicorn==0.23.2**: ASGI server
- **torch==2.0.1**: Deep learning framework
- **transformers==4.33.0**: Hugging Face models
- **datasets==2.14.0**: Data loading and processing
- **scikit-learn==1.3.0**: ML utilities and metrics
- **numpy**: Numerical computing
- **pandas**: Data manipulation
- **pydantic**: Data validation
- **accelerate**: Multi-GPU/distributed training

### Development (requirements-dev.txt)
- **pytest**: Testing framework
- **black**: Code formatter
- **flake8**: Linter
- **mypy**: Type checking
- **jupyter**: Interactive notebooks

---

## Success Criteria

### Model Performance
- âœ… Accuracy: >90% overall
- âœ… Macro F1: >0.90
- âœ… All intent classes: >0.85 F1
- âœ… Emergency classification: >95% (critical for safety)

### System Performance
- âœ… Latency P95: <50ms
- âœ… Latency P99: <60ms
- âœ… Throughput: >20 texts/sec
- âœ… Availability: >99.9%

### Code Quality
- âœ… Type hints on all functions
- âœ… Unit test coverage: >85%
- âœ… Integration tests: All critical paths
- âœ… Documentation: API + deployment guide

### Deployment Ready
- âœ… Docker image builds successfully
- âœ… Health checks passing
- âœ… Graceful shutdown handling
- âœ… Fallback logic implemented and tested

---

## Timeline

| Phase | Dates | Tasks | Status |
|-------|-------|-------|--------|
| **1** | Day 1-2 | Infrastructure setup | âœ… COMPLETE |
| **2** | Day 3-5 | Model training & evaluation | â³ PENDING |
| **3** | Day 6-7 | Backend integration | â³ PENDING |
| **4** | Day 8-10 | Testing & deployment | â³ PENDING |

---

## Next Actions

### Immediate (Next 30 minutes)
1. Review this implementation guide
2. Read [README.md](README.md) for API details
3. Verify Python 3.11+ installed: `python --version`

### Within 24 hours
1. Run setup script to install dependencies
2. Run `prepare_data.py` to split datasets
3. Start model training with `python train.py`

### Monitoring Training
- Watch training logs for loss convergence
- Check validation metrics improve each epoch
- Monitor GPU memory (should stabilize by epoch 2)
- Estimated completion time will display after ~5 minutes

### After Training (Day 5)
1. Run `evaluate.py` to get final metrics
2. Review metrics.json for accuracy scores
3. If accuracy >90%, proceed to backend integration
4. If accuracy <90%, consider:
   - Using a larger training dataset
   - Increasing training epochs
   - Adjusting learning rate
   - Using a larger pretrained model

---

## Troubleshooting

### Common Issues

**Import Errors After Setup**
```bash
# Verify venv activated
which python  # Should show venv path

# Reinstall requirements
pip install --force-reinstall -r requirements.txt
```

**CUDA/GPU Issues**
```bash
# Check CUDA availability
python -c "import torch; print(torch.cuda.is_available())"

# Force CPU training
export CUDA_VISIBLE_DEVICES=""
python train.py
```

**Out of Memory During Training**
```bash
# Reduce batch size in config.py
TRAINING_CONFIG["batch_size"] = 8  # Default: 16

# Or reduce model max_length
MODEL_CONFIG["max_length"] = 256  # Default: 512
```

**Model Download Taking Long**
First inference downloads model (~500MB) from Hugging Face. To pre-download:
```bash
python -c "from transformers import AutoModel; AutoModel.from_pretrained('BiomedBERT')"
```

---

## References

- **BiomedBERT Model**: https://huggingface.co/dmis-lab/biobert-base-cased-v1.1
- **FastAPI Documentation**: https://fastapi.tiangolo.com
- **Hugging Face Transformers**: https://huggingface.co/transformers
- **PyTorch Documentation**: https://pytorch.org/docs
- **Clinical Intent Classification Paper**: [Reference] (add paper if available)

---

## Contact & Support

For questions or issues:
1. Check README.md for common questions
2. Review error messages carefully
3. Check GPU memory and disk space
4. Verify Python version compatibility

---

**Last Updated**: Today
**Prepared By**: CareDroid AI Team
**Batch**: Batch 9 - Advanced NLU with Fine-Tuned BERT
